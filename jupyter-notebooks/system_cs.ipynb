{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: 1) A system NLG database (containing system utterances along with conversational strategies)\n",
    "# Input: 2) Dialog transcripts containing user and system turns. \n",
    "# Output: Each system turn tagged with an appropriate set of conversational strategies\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import json\n",
    "\n",
    "parent_path = os.path.abspath('../')\n",
    "map_to_none = ['BC', 'OUT', 'REC', 'RSE', 'o']\n",
    "data_dir = parent_path + '/data/davos/'\n",
    "nlg_db_fname = 'nlg_database.csv'\n",
    "sentence_col_name = 'SENTENCE'\n",
    "\n",
    "df = pd.read_csv(data_dir + nlg_db_fname)\n",
    "utterances = df[sentence_col_name].tolist()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "tokenized_utterances = []\n",
    "clean_utterances = []\n",
    "for i, u in enumerate(utterances):\n",
    "    # Remove punctuations and make all characters lower case\n",
    "    cu = u.translate(translator).lower()\n",
    "    clean_utterances.append(cu)\n",
    "    tu = word_tokenize(cu)\n",
    "    tokenized_utterances.append(tu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['514007', '510275', '1001444', '510881', '2016938', '513033', '1001335', '514819', '1002013', '513024', '1001877', '513961', '510918', '510688', '510500', '516417', '2001299', '509907', '1002648', '510410', '515233', '515151', '511862', '511447', '513324', '511250', '516517', '2004828', '2002040', '1001636', '510398', '510401', '509969', '513058', '1002655', '510196', '515762', '509997', '516135', '2005133', '2008165', '511512', '1003158', '511665', '510999', '1002281', '510422', '1002242', '2001541', '1003016', '514928', '513935', '513210', '509921', '513762', '512508', '510849', '514232', '1002422', '515289', '2005036', '510910', '513969', '510457', '1001918', '2001521', '510734', '515843']\n"
     ]
    }
   ],
   "source": [
    "transcripts_dir = data_dir + 'transcripts/'\n",
    "processed_files_dir = transcripts_dir + 'processed/'\n",
    "agent_cs_dir = data_dir + 'agent_cs/'\n",
    "txt_suffix = '.txt'\n",
    "\n",
    "def make_dir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "all_files = os.listdir(transcripts_dir)\n",
    "session_ids = []\n",
    "for file in all_files:\n",
    "    if file.endswith(txt_suffix):\n",
    "        session_ids.append(file.split(txt_suffix)[0])\n",
    "\n",
    "make_dir(processed_files_dir)\n",
    "make_dir(agent_cs_dir)\n",
    "print(session_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting potential matches for file 514007.txt...Session: 514007, Row: 10, Repeated by: sara\n",
      "['SARA', ' 01:13.1', ' I can recommend to you people to meet or sessions to attend', ' 01:16.2']\n",
      "Session: 514007, Row: 26, Repeated by: user\n",
      "['User', ' 03:45.5', ' nothing', ' 03:45.7']\n",
      "Session: 514007, Row: 29, Repeated by: user\n",
      "['User', ' 04:01.6', ' ah um hmm', ' 04:02.7']\n",
      "Done.\n",
      "Getting potential matches for file 510275.txt...Done.\n",
      "Getting potential matches for file 1001444.txt...Done.\n",
      "Getting potential matches for file 510881.txt...Done.\n",
      "Getting potential matches for file 2016938.txt...Done.\n",
      "Getting potential matches for file 513033.txt...Done.\n",
      "Getting potential matches for file 1001335.txt...Done.\n",
      "Getting potential matches for file 514819.txt...Done.\n",
      "Getting potential matches for file 1002013.txt...Done.\n",
      "Getting potential matches for file 513024.txt...Done.\n",
      "Getting potential matches for file 1001877.txt...Done.\n",
      "Getting potential matches for file 513961.txt...Done.\n",
      "Getting potential matches for file 510918.txt...Done.\n",
      "Getting potential matches for file 510688.txt...Done.\n",
      "Getting potential matches for file 510500.txt...Done.\n",
      "Getting potential matches for file 516417.txt...Done.\n",
      "Getting potential matches for file 2001299.txt...Done.\n",
      "Getting potential matches for file 509907.txt...Done.\n",
      "Getting potential matches for file 1002648.txt...Done.\n",
      "Getting potential matches for file 510410.txt...Done.\n",
      "Getting potential matches for file 515233.txt...Done.\n",
      "Getting potential matches for file 515151.txt...Done.\n",
      "Getting potential matches for file 511862.txt...Done.\n",
      "Getting potential matches for file 511447.txt...Done.\n",
      "Getting potential matches for file 513324.txt...Done.\n",
      "Getting potential matches for file 511250.txt...Done.\n",
      "Getting potential matches for file 516517.txt...Session: 516517, Row: 10, Repeated by: user\n",
      "['User', ' 02:11.9', ' yes', ' 02:12.1']\n",
      "Done.\n",
      "Getting potential matches for file 2004828.txt...Done.\n",
      "Getting potential matches for file 2002040.txt...Done.\n",
      "Getting potential matches for file 1001636.txt...Done.\n",
      "Getting potential matches for file 510398.txt...Done.\n",
      "Getting potential matches for file 510401.txt...Done.\n",
      "Getting potential matches for file 509969.txt...Done.\n",
      "Getting potential matches for file 513058.txt...Done.\n",
      "Getting potential matches for file 1002655.txt...Done.\n",
      "Getting potential matches for file 510196.txt...Done.\n",
      "Getting potential matches for file 515762.txt...Done.\n",
      "Getting potential matches for file 509997.txt...Done.\n",
      "Getting potential matches for file 516135.txt...Done.\n",
      "Getting potential matches for file 2005133.txt...Done.\n",
      "Getting potential matches for file 2008165.txt...Done.\n",
      "Getting potential matches for file 511512.txt...Done.\n",
      "Getting potential matches for file 1003158.txt...Done.\n",
      "Getting potential matches for file 511665.txt...Done.\n",
      "Getting potential matches for file 510999.txt...Done.\n",
      "Getting potential matches for file 1002281.txt...Done.\n",
      "Getting potential matches for file 510422.txt...Done.\n",
      "Getting potential matches for file 1002242.txt...Done.\n",
      "Getting potential matches for file 2001541.txt...Done.\n",
      "Getting potential matches for file 1003016.txt...Done.\n",
      "Getting potential matches for file 514928.txt...Done.\n",
      "Getting potential matches for file 513935.txt...Done.\n",
      "Getting potential matches for file 513210.txt...Done.\n",
      "Getting potential matches for file 509921.txt...Done.\n",
      "Getting potential matches for file 513762.txt...Done.\n",
      "Getting potential matches for file 512508.txt...Done.\n",
      "Getting potential matches for file 510849.txt...Done.\n",
      "Getting potential matches for file 514232.txt...Done.\n",
      "Getting potential matches for file 1002422.txt...Done.\n",
      "Getting potential matches for file 515289.txt...Done.\n",
      "Getting potential matches for file 2005036.txt...Done.\n",
      "Getting potential matches for file 510910.txt...Done.\n",
      "Getting potential matches for file 513969.txt...Done.\n",
      "Getting potential matches for file 510457.txt...Done.\n",
      "Getting potential matches for file 1001918.txt...Done.\n",
      "Getting potential matches for file 2001521.txt...Done.\n",
      "Getting potential matches for file 510734.txt...Done.\n",
      "Getting potential matches for file 515843.txt...Done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import operator\n",
    "import time\n",
    "\n",
    "matches = []\n",
    "sys_turns = []\n",
    "step = 0.05\n",
    "base_thresh = 0.8\n",
    "options_suffix = '_options.csv'\n",
    "agent_name = 'SARA'\n",
    "time_format = '%M:%S.%f'\n",
    "time_alt_format = '%M:%S:%f'\n",
    "min_to_sec = 60\n",
    "min_id = 4\n",
    "sec_id = 5\n",
    "agent_name_col = 0\n",
    "agent_turn_col = 2\n",
    "\n",
    "participants = ['sara', 'user']\n",
    "line_num = {}\n",
    "\n",
    "for sid in session_ids:\n",
    "    print(\"Getting potential matches for file %s.txt...\" % (sid), end='')\n",
    "    \n",
    "    prev = None\n",
    "    curr = None\n",
    "    for p in participants:\n",
    "        line_num[p] = 0\n",
    "    \n",
    "    options_fname = processed_files_dir + sid + options_suffix\n",
    "    if os.path.exists(options_fname):\n",
    "        os.remove(options_fname)\n",
    "    fw = open(options_fname, 'w', encoding='utf-8-sig')\n",
    "\n",
    "    with open(transcripts_dir + sid + txt_suffix, 'r') as fr:\n",
    "        rows = fr.readlines()\n",
    "        line = 0\n",
    "        for rid, r in enumerate(rows):\n",
    "            r = r.rstrip(\"\\n\")\n",
    "            r = r.split(\",\")\n",
    "            line += 1\n",
    "            \n",
    "            curr = r[agent_name_col].strip().lower()\n",
    "            if curr in participants:\n",
    "                line_num[curr] += 1\n",
    "                if curr == prev:\n",
    "                    print(\"Session: %s, Row: %d, Repeated by: %s\" %(sid, line_num[curr]-1, curr))\n",
    "                    print(r)\n",
    "            prev = curr\n",
    "            \n",
    "            if r[agent_name_col] == agent_name:\n",
    "                # Picking end of turn (last column) as the time stamp for the turn\n",
    "                try:\n",
    "                    struct_t = time.strptime(r[-1].strip(), time_format)\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        struct_t = time.strptime(r[-1].strip(), time_alt_format)\n",
    "                    except ValueError:\n",
    "                        print(r)\n",
    "                    \n",
    "                tsec = struct_t[min_id] * min_to_sec + struct_t[sec_id]\n",
    "                fw.write(str(tsec) + \",\")\n",
    "\n",
    "                clean_sys_turn = str(r[agent_turn_col].strip().translate(translator).lower())\n",
    "                tokenized_sys_turn = word_tokenize(clean_sys_turn)\n",
    "                fw.write(clean_sys_turn + \", \")\n",
    "                n_sys_turn = len(tokenized_sys_turn)\n",
    "                \n",
    "                scores = {}\n",
    "                for i, tu in enumerate(tokenized_utterances):\n",
    "                    n_utterance = len(tu)\n",
    "                    n_common = len(set(tokenized_sys_turn).intersection(set(tu)))\n",
    "                    scores[i] = (n_common/n_utterance, n_utterance)\n",
    "                \n",
    "                sorted_scores = sorted(scores.items(), key= lambda x: x[1][0], reverse=True)\n",
    "                potential_matches = []\n",
    "                thresh = base_thresh\n",
    "                while len(potential_matches) == 0:\n",
    "                    potential_matches = sorted([s for s in sorted_scores if s[1][0] >= thresh], key= lambda x: x[1][1], reverse=True)\n",
    "                    thresh -= step\n",
    "            \n",
    "                # Write to file.\n",
    "                for i, pm in enumerate(potential_matches):\n",
    "                    end_str = \",\" if i < len(potential_matches)-1 else \"\\n\"\n",
    "                    fw.write(clean_utterances[pm[0]] + \": \" + str(pm[0]) + end_str)   \n",
    "    fw.close()\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"acknowledgement\": \"ack()\",\n",
      "  \"feedback_person_recommendation_1st_time_yes\": \"glad()\",\n",
      "  \"request(send_msg_tlink)\": \"request(send_msg_tlink)\",\n",
      "  \"outcome_person_recommendation_1st_time\": \"inform(info)\",\n",
      "  \"feedback_session_recommendation_2nd_time_no\": \"sorry()\",\n",
      "  \"do_selfie\": \"take_selfie()\",\n",
      "  \"ready_selfie\": \"take_selfie()\",\n",
      "  \"farewell\": \"bye()\",\n",
      "  \"do_attendance_elicitation\": \"request(first_time)\",\n",
      "  \"greeting\": \"greeting()\",\n",
      "  \"finish_selfie\": \"take_selfie()\",\n",
      "  \"feedback_interest_elicitation_session_recommendation\": \"give_feedback()\",\n",
      "  \"elicit_feedback_person_recommendation_1st_time\": \"request(feedback)\",\n",
      "  \"feedback_goal_elicitation\": \"give_feedback()\",\n",
      "  \"pleasure_coming_together\": \"greeting()\",\n",
      "  \"do_person_recommendation_1st_time\": \"request(goal)\",\n",
      "  \"start_goal_elicitation\": \"request(primary_goal)\",\n",
      "  \"do_interest_elicitation_session_recommendation\": \"request(interest)\",\n",
      "  \"introduce\": \"introduce()\",\n",
      "  \"pre_closing\": \"request(anything_else)\",\n",
      "  \"bye\": \"bye()\",\n",
      "  \"elicit_feedback_session_recommendation_1st_time\": \"request(feedback)\",\n",
      "  \"start_person_recommendation_1st_time\": \"request(goal)\",\n",
      "  \"do_session_recommendation_1st_time\": \"request(goal)\",\n",
      "  \"selfiebusy\": \"request(selfie)\",\n",
      "  \"takecare\": \"bye()\",\n",
      "  \"request(selfie)\": \"request(selfie)\",\n",
      "  \"feedback_session_recommendation_1st_time_yes\": \"glad()\",\n",
      "  \"do_goal_elicitation\": \"request(primary_goal)\",\n",
      "  \"feedback_attendance_elicitation_not_first\": \"give_feedback()\",\n",
      "  \"feedback_attendance_elicitation_first\": \"give_feedback()\",\n",
      "  \"start_person_recommendation_2nd_time_if_prior_feedback_yes\": \"request(another_reco)\",\n",
      "  \"other()\": \"other()\",\n",
      "  \"start_session_recommendation_1st_time\": \"request(goal)\",\n",
      "  \"do_person_recommendation_2nd_time\": \"request(another_reco)\",\n",
      "  \"launch_food_recommendation\": \"request(goal)\",\n",
      "  \"do_food_recommendation\": \"request(goal)\",\n",
      "  \"outcome_session_recommendation_3rd_time\": \"inform(info)\",\n",
      "  \"launch_person_recommendation\": \"request(goal)\",\n",
      "  \"outcome_session_recommendation_1st_time\": \"inform(info)\",\n",
      "  \"start_session_recommendation_2nd_time_if_prior_feedback_yes\": \"request(another_reco)\",\n",
      "  \"end_session_recommendation_1st_time_yes\": \"send_msg()\",\n",
      "  \"request(feedback)\": \"request(feedback)\",\n",
      "  \"launch_session_recommendation\": \"request(goal)\",\n",
      "  \"elicit_feedback_food_recommendation\": \"request(feedback)\",\n",
      "  \"moodresponse\": \"you()\",\n",
      "  \"feedback_session_recommendation_2nd_time_yes\": \"glad()\",\n",
      "  \"inform(info)\": \"inform(info)\",\n",
      "  \"request(another_reco)\": \"request(another_reco)\",\n",
      "  \"out_of_domain\": \"other()\",\n",
      "  \"outcome_food_recommendation\": \"inform(info)\",\n",
      "  \"outcome_session_recommendation_2nd_time\": \"inform(info)\",\n",
      "  \"end_person_recommendation_2nd_time\": \"request(send_msg_tlink)\",\n",
      "  \"feedback_food_recommendation_yes\": \"glad()\",\n",
      "  \"matchmaker\": \"introduce()\",\n",
      "  \"send_msg()\": \"send_msg()\",\n",
      "  \"tired\": \"tired()\",\n",
      "  \"end_person_recommendation_1st_time_no\": \"no_worries()\",\n",
      "  \"greeting()\": \"greeting()\",\n",
      "  \"quick_response_recovery\": \"other()\",\n",
      "  \"end_person_recommendation_1st_time_yes\": \"send_msg()\",\n",
      "  \"feedback_person_recommendation_2nd_time_yes\": \"glad()\",\n",
      "  \"request(interest)\": \"request(interest)\",\n",
      "  \"thank\": \"thank()\",\n",
      "  \"do_session_recommendation_2nd_time\": \"do()\",\n",
      "  \"elicit_feedback_session_recommendation_2nd_time\": \"request(feedback)\",\n",
      "  \"end_session_recommendation_1st_time\": \"request(send_msg_tlink)\",\n",
      "  \"outcome_person_recommendation_2nd_time\": \"inform(info)\",\n",
      "  \"overwhelmed\": \"tired()\",\n",
      "  \"pleasuretomeet\": \"greeting()\",\n",
      "  \"start_attendance_elicitation\": \"request(first_time)\",\n",
      "  \"start_session_recommendation_2nd_time_if_prior_feedback_no\": \"request(another_reco)\",\n",
      "  \"end_person_recommendation_1st_time_no_extra_time\": \"request(send_msg_tlink)\",\n",
      "  \"interests\": \"request(interest)\",\n",
      "  \"start_interest_elicitation_session_recommendation\": \"request(interest)\",\n",
      "  \"end_person_recommendation_2nd_time_yes\": \"send_msg()\",\n",
      "  \"introduceself\": \"introduce()\",\n",
      "  \"do_session_recommendation_3rd_time\": \"do()\",\n",
      "  \"end_person_recommendation_1st_time\": \"request(send_msg_tlink)\",\n",
      "  \"feedback_start_person_recommendation_2nd_time_yes\": \"give_feedback()\",\n",
      "  \"start_food_recommendation\": \"request(goal)\",\n",
      "  \"start_person_recommendation_2nd_time_if_prior_feedback_no\": \"request(another_reco)\",\n",
      "  \"ask_selfie\": \"request(selfie)\",\n",
      "  \"elicit_feedback_party_recommendation\": \"request(feedback)\",\n",
      "  \"end_session_recommendation_2nd_time_yes\": \"send_msg()\",\n",
      "  \"feedback_session_recommendation_1st_time_no\": \"sorry()\",\n",
      "  \"firstthings\": \"request(interest)\",\n",
      "  \"introduce()\": \"introduce()\",\n",
      "  \"request(goal)\": \"request(goal)\",\n",
      "  \"unsure\": \"other()\",\n",
      "  \"bye()\": \"bye()\",\n",
      "  \"do_party_recommendation_Tuesday\": \"do()\",\n",
      "  \"elicit_feedback_person_recommendation_2nd_time\": \"request(feedback)\",\n",
      "  \"end_session_recommendation_2nd_time\": \"request(send_msg_tlink)\",\n",
      "  \"feedback_goal_not_identified\": \"request(primary_goal)\",\n",
      "  \"Not found\": \"send_msg()\",\n",
      "  \"party2\": \"inform(info)\",\n",
      "  \"start_person_recommendation_3rd_time_if_prior_feedback_yes\": \"request(another_reco)\",\n",
      "  \"end_session_recommendation_1st_time_no\": \"no_worries()\",\n",
      "  \"feedback_person_recommendation_1st_time_no\": \"sorry()\",\n",
      "  \"feedback_person_recommendation_2nd_time_no\": \"sorry()\",\n",
      "  \"feedback_person_recommendation_3rd_time_yes\": \"glad()\",\n",
      "  \"feedback_session_recommendation_3rd_time_no\": \"sorry()\",\n",
      "  \"feedback_start_person_recommendation_3rd_time_yes\": \"give_feedback()\",\n",
      "  \"havefun\": \"other()\",\n",
      "  \"howsgoing\": \"greeting()\",\n",
      "  \"ifbody\": \"other()\",\n",
      "  \"knowthem\": \"request(feedback)\",\n",
      "  \"launch_party_recommendation\": \"request(goal)\",\n",
      "  \"outcome_person_recommendation_3rd_time\": \"inform(info)\",\n",
      "  \"reason_recommendation\": \"other()\",\n",
      "  \"request(primary_goal)\": \"request(primary_goal)\",\n",
      "  \"secret\": \"other()\",\n",
      "  \"sogood\": \"other()\",\n",
      "  \"start_session_recommendation_3rd_time_if_prior_feedback_no\": \"request(another_reco)\",\n",
      "  \"work\": \"request(interest)\",\n",
      "  \"elicit_feedback_davos_recommendation\": \"request(feedback)\",\n",
      "  \"end_person_recommendation_2nd_time_no\": \"no_worries()\",\n",
      "  \"end_session_recommendation_3rd_time\": \"request(send_msg_tlink)\",\n",
      "  \"end_session_recommendation_3rd_time_no\": \"no_worries()\",\n",
      "  \"feedback_interest_elicitation_session_recommendation_alternate_job\": \"give_feedback()\",\n",
      "  \"give_feedback()\": \"give_feedback()\",\n",
      "  \"hobby\": \"other()\",\n",
      "  \"intromessage\": \"request(send_msg_tlink)\",\n",
      "  \"lunch\": \"inform(info)\",\n",
      "  \"nopress\": \"no_worries()\",\n",
      "  \"onitsway\": \"other()\",\n",
      "  \"request(anything_else)\": \"request(anything_else)\",\n",
      "  \"searchbrain\": \"other()\",\n",
      "  \"soundgood\": \"other()\",\n",
      "  \"start_interest_elicitation_person_recommendation\": \"request(interest)\",\n",
      "  \"start_interest_elicitation_session_recommendation_return_visitor\": \"request(interest)\",\n",
      "  \"take_selfie()\": \"take_selfie()\",\n",
      "  \"task_inability_session\": \"other()\",\n",
      "  \"tired()\": \"tired()\",\n",
      "  \"transition\": \"other()\",\n",
      "  \"whatsay\": \"request(feedback)\",\n",
      "  \"adventureifknow\": \"do()\",\n",
      "  \"allday\": \"other()\",\n",
      "  \"ask_met_before\": \"request(met_before)\",\n",
      "  \"do_person_recommendation_3rd_time\": \"do()\",\n",
      "  \"elicit_feedback_person_recommendation_3rd_time\": \"request(feedback)\",\n",
      "  \"elicit_feedback_session_recommendation_3rd_time\": \"request(feedback)\",\n",
      "  \"end_session_recommendation_3rd_time_yes\": \"send_msg()\",\n",
      "  \"happily\": \"other()\",\n",
      "  \"howabout\": \"request(feedback)\",\n",
      "  \"introducenew\": \"request(send_msg_tlink)\",\n",
      "  \"introduct()\": \"introduce()\",\n",
      "  \"lipssealed\": \"other()\",\n",
      "  \"messagecontent\": \"inform(info)\",\n",
      "  \"No found\": \"other()\",\n",
      "  \"request(first_time)\": \"request(first_time)\",\n",
      "  \"sara_how_old\": \"other()\",\n",
      "  \"sara_where_from\": \"other()\",\n",
      "  \"seshemail\": \"request(send_msg_tlink)\",\n",
      "  \"start_party_recommendation\": \"request(goal)\",\n",
      "  \"start_session_recommendation_3rd_time_if_prior_feedback_yes\": \"request(another_reco)\"\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'act'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/prelim-analysis-research/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'act'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-ae059a1ed077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mintention_act_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'col_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'act'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintention_act_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'act'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_acts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/prelim-analysis-research/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/prelim-analysis-research/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/prelim-analysis-research/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/prelim-analysis-research/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/prelim-analysis-research/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'act'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "df_map = pd.read_csv(data_dir + 'all_task_intentions_act_map_final.csv')\n",
    "intention_act_map = dict(zip(df_map['col_1'], df_map['act']))\n",
    "print(json.dumps(intention_act_map, indent=2))\n",
    "all_acts = list(set(df['act']))\n",
    "print(all_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CSs for file 514007_final.csv...Done.\n",
      "Getting CSs for file 510275_final.csv...Done.\n",
      "Getting CSs for file 1001444_final.csv...Done.\n",
      "Getting CSs for file 510881_final.csv...Done.\n",
      "Getting CSs for file 2016938_final.csv...Done.\n",
      "Getting CSs for file 513033_final.csv...Done.\n",
      "Getting CSs for file 1001335_final.csv...Done.\n",
      "Getting CSs for file 514819_final.csv...Done.\n",
      "Getting CSs for file 1002013_final.csv...Done.\n",
      "Getting CSs for file 513024_final.csv...Done.\n",
      "Getting CSs for file 1001877_final.csv...Done.\n",
      "Getting CSs for file 513961_final.csv...Done.\n",
      "Getting CSs for file 510918_final.csv...Done.\n",
      "Getting CSs for file 510688_final.csv...Done.\n",
      "Getting CSs for file 510500_final.csv...Done.\n",
      "Getting CSs for file 516417_final.csv...Done.\n",
      "Getting CSs for file 2001299_final.csv...Done.\n",
      "Getting CSs for file 509907_final.csv...Done.\n",
      "Getting CSs for file 1002648_final.csv...Done.\n",
      "Getting CSs for file 510410_final.csv...Done.\n",
      "Getting CSs for file 515233_final.csv...Done.\n",
      "Getting CSs for file 515151_final.csv...Done.\n",
      "Getting CSs for file 511862_final.csv...Done.\n",
      "Getting CSs for file 511447_final.csv...Done.\n",
      "Getting CSs for file 513324_final.csv...Done.\n",
      "Getting CSs for file 511250_final.csv...Done.\n",
      "Getting CSs for file 516517_final.csv...Done.\n",
      "Getting CSs for file 2004828_final.csv...Done.\n",
      "Getting CSs for file 2002040_final.csv...Done.\n",
      "Getting CSs for file 1001636_final.csv...Done.\n",
      "Getting CSs for file 510398_final.csv...Done.\n",
      "Getting CSs for file 510401_final.csv...Done.\n",
      "Getting CSs for file 509969_final.csv...Done.\n",
      "Getting CSs for file 513058_final.csv...Done.\n",
      "Getting CSs for file 1002655_final.csv...Done.\n",
      "Getting CSs for file 510196_final.csv...Done.\n",
      "Getting CSs for file 515762_final.csv...Done.\n",
      "Getting CSs for file 509997_final.csv...Done.\n",
      "Getting CSs for file 516135_final.csv...Done.\n",
      "Getting CSs for file 2005133_final.csv...Done.\n",
      "Getting CSs for file 2008165_final.csv...Done.\n",
      "Getting CSs for file 511512_final.csv...Done.\n",
      "Getting CSs for file 1003158_final.csv...Done.\n",
      "Getting CSs for file 511665_final.csv...Done.\n",
      "Getting CSs for file 510999_final.csv...Done.\n",
      "Getting CSs for file 1002281_final.csv...Done.\n",
      "Getting CSs for file 510422_final.csv...Done.\n",
      "Getting CSs for file 1002242_final.csv...Done.\n",
      "Getting CSs for file 2001541_final.csv...Done.\n",
      "Getting CSs for file 1003016_final.csv...Done.\n",
      "Getting CSs for file 514928_final.csv...0\n",
      "greeting\n",
      "['greeting()']\n",
      "1\n",
      "pleasure_coming_together\n",
      "['greeting()']\n",
      "2\n",
      "introduce\n",
      "['introduce()']\n",
      "2\n",
      "do_attendance_elicitation\n",
      "['introduce()', 'request(first_time)']\n",
      "3\n",
      "start_goal_elicitation\n",
      "['request(primary_goal)']\n",
      "3\n",
      "feedback_attendance_elicitation_not_first\n",
      "['request(primary_goal)', 'give_feedback()']\n",
      "4\n",
      "do_interest_elicitation_session_recommendation\n",
      "['request(interest)']\n",
      "4\n",
      "feedback_goal_elicitation\n",
      "['request(interest)', 'give_feedback()']\n",
      "5\n",
      "launch_session_recommendation\n",
      "['request(goal)']\n",
      "5\n",
      "feedback_interest_elicitation_session_recommendation\n",
      "['request(goal)', 'give_feedback()']\n",
      "6\n",
      "outcome_session_recommendation_3rd_time\n",
      "['inform(info)']\n",
      "7\n",
      "elicit_feedback_session_recommendation_1st_time\n",
      "['request(feedback)']\n",
      "8\n",
      "feedback_session_recommendation_1st_time_yes\n",
      "['glad()']\n",
      "10\n",
      "launch_person_recommendation\n",
      "['request(goal)']\n",
      "11\n",
      "outcome_person_recommendation_1st_time\n",
      "['inform(info)']\n",
      "11\n",
      "do_person_recommendation_1st_time\n",
      "['inform(info)', 'request(goal)']\n",
      "12\n",
      "outcome_person_recommendation_1st_time\n",
      "['inform(info)']\n",
      "12\n",
      "feedback_person_recommendation_1st_time_yes\n",
      "['inform(info)', 'glad()']\n",
      "12\n",
      "elicit_feedback_person_recommendation_1st_time\n",
      "['inform(info)', 'glad()', 'request(feedback)']\n",
      "13\n",
      "feedback_person_recommendation_1st_time_yes\n",
      "['glad()']\n",
      "14\n",
      "acknowledgement\n",
      "['ack()']\n",
      "15\n",
      "nopress\n",
      "['no_worries()']\n",
      "17\n",
      "start_person_recommendation_2nd_time_if_prior_feedback_yes\n",
      "['request(another_reco)']\n",
      "18\n",
      "outcome_person_recommendation_1st_time\n",
      "['inform(info)']\n",
      "18\n",
      "feedback_start_person_recommendation_2nd_time_yes\n",
      "['inform(info)', 'give_feedback()']\n",
      "19\n",
      "launch_food_recommendation\n",
      "['request(goal)']\n",
      "19\n",
      "feedback_session_recommendation_2nd_time_no\n",
      "['request(goal)', 'sorry()']\n",
      "20\n",
      "launch_food_recommendation\n",
      "['request(goal)']\n",
      "21\n",
      "lunch\n",
      "['inform(info)']\n",
      "21\n",
      "do_food_recommendation\n",
      "['inform(info)', 'request(goal)']\n",
      "22\n",
      "elicit_feedback_food_recommendation\n",
      "['request(feedback)']\n",
      "24\n",
      "ready_selfie\n",
      "['take_selfie()']\n",
      "25\n",
      "finish_selfie\n",
      "['take_selfie()']\n",
      "25\n",
      "pre_closing\n",
      "['take_selfie()', 'request(anything_else)']\n",
      "25\n",
      "do_selfie\n",
      "['take_selfie()', 'request(anything_else)', 'take_selfie()']\n",
      "26\n",
      "farewell\n",
      "['bye()']\n",
      "Done.\n",
      "Getting CSs for file 513935_final.csv...Done.\n",
      "Getting CSs for file 513210_final.csv...Done.\n",
      "Getting CSs for file 509921_final.csv...Done.\n",
      "Getting CSs for file 513762_final.csv...Done.\n",
      "Getting CSs for file 512508_final.csv...Done.\n",
      "Getting CSs for file 510849_final.csv...Done.\n",
      "Getting CSs for file 514232_final.csv...Done.\n",
      "Getting CSs for file 1002422_final.csv...Done.\n",
      "Getting CSs for file 515289_final.csv...Done.\n",
      "Getting CSs for file 2005036_final.csv...Done.\n",
      "Getting CSs for file 510910_final.csv...Done.\n",
      "Getting CSs for file 513969_final.csv...Done.\n",
      "Getting CSs for file 510457_final.csv...Done.\n",
      "Getting CSs for file 1001918_final.csv...Done.\n",
      "Getting CSs for file 2001521_final.csv...Done.\n",
      "Getting CSs for file 510734_final.csv...Done.\n",
      "Getting CSs for file 515843_final.csv...Done.\n",
      "338\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "final_files_dir = transcripts_dir + 'final/'\n",
    "final_suffix = '_final.csv'\n",
    "tsec_col = 0\n",
    "matches_col_start = 2\n",
    "utterance_id = 1\n",
    "agent_timestamp_suffix = '_agent_timestamps.pkl'\n",
    "cs_suffix = '_agent_cs.pkl'\n",
    "intention_suffix = '_agent_intention.pkl'\n",
    "all_agent_task_intentions = []\n",
    "count_no_matches = 0\n",
    "\n",
    "for sid in session_ids:\n",
    "    print(\"Getting CSs for file %s_final.csv...\" % (sid), end='')\n",
    "\n",
    "    with open(final_files_dir + sid + final_suffix, 'r', encoding='utf-8-sig', errors='ignore') as fr:\n",
    "        rows = fr.readlines()\n",
    "        tsec = {}\n",
    "        cs = {}\n",
    "        task_intention = {}\n",
    "        for i, r in enumerate(rows):\n",
    "            r = r.rstrip(\"\\n\")\n",
    "            r = r.split(\",\")\n",
    "            tsec[i] = int(r[tsec_col])\n",
    "            matches = r[matches_col_start:]\n",
    "            n_matches = 0\n",
    "            for m in matches:\n",
    "                if m != '':\n",
    "                    n_matches += 1\n",
    "            \n",
    "            for m in matches:\n",
    "                if m != '':\n",
    "                    if i not in cs.keys():\n",
    "                        cs[i] = []\n",
    "                        task_intention[i] = []\n",
    "                    m_id = m.split(':')[utterance_id].strip()\n",
    "                    try:\n",
    "                        cs_candidate = df.loc[int(m_id)]['STRATEGY']\n",
    "                        task_intention_candidate = df.loc[int(m_id)]['SYSTEM_INTENTION']\n",
    "                        cs[i].append('NONE' if cs_candidate in map_to_none else cs_candidate)\n",
    "                        # all_agent_task_intentions.append(task_intention_candidate)\n",
    "                        task_intention[i].append(intention_act_map[task_intention_candidate])\n",
    "                        if sid == '514928':\n",
    "                            print(i)\n",
    "                            print(task_intention_candidate)\n",
    "                            print(task_intention[i])\n",
    "                    except ValueError:\n",
    "                        # print(r)\n",
    "                        task_intention_candidate = m.split(':')[0].strip()\n",
    "                        if task_intention_candidate != 'No match':\n",
    "                            count_no_matches += 1\n",
    "                            # all_agent_task_intentions.append(task_intention_candidate)\n",
    "                            task_intention[i].append(intention_act_map[task_intention_candidate])\n",
    "                        cs[i].append(m_id)\n",
    "                        \n",
    "    \n",
    "    with open(agent_cs_dir + sid + agent_timestamp_suffix, 'wb') as f:\n",
    "        pickle.dump(tsec, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(agent_cs_dir + sid + cs_suffix, 'wb') as f:\n",
    "        pickle.dump(cs, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open(agent_cs_dir + sid + intention_suffix, 'wb') as f:\n",
    "        pickle.dump(task_intention, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"Done.\")\n",
    "print(count_no_matches)\n",
    "# counter_dict = dict(Counter(all_agent_task_intentions))\n",
    "# df_new = pd.DataFrame({'col_1': list(counter_dict.keys()), 'col_2': list(counter_dict.values())})\n",
    "# df_new.to_csv(data_dir + 'all_task_intentions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting user turn timestamps for session 1001444...Done.\n",
      "Getting user turn timestamps for session 2016938...Done.\n",
      "Getting user turn timestamps for session 1001335...Done.\n",
      "Getting user turn timestamps for session 1002013...Done.\n",
      "Getting user turn timestamps for session 1001877...Done.\n",
      "Getting user turn timestamps for session 516417...Done.\n",
      "Getting user turn timestamps for session 2001299...Done.\n",
      "Getting user turn timestamps for session 1002648...Done.\n",
      "Getting user turn timestamps for session 515233...Done.\n",
      "Getting user turn timestamps for session 515151...Done.\n",
      "Getting user turn timestamps for session 516517...Done.\n",
      "Getting user turn timestamps for session 2004828...Done.\n",
      "Getting user turn timestamps for session 2002040...Done.\n",
      "Getting user turn timestamps for session 1001636...Done.\n",
      "Getting user turn timestamps for session 1002655...Done.\n",
      "Getting user turn timestamps for session 515762...Done.\n",
      "Getting user turn timestamps for session 516135...Done.\n",
      "Getting user turn timestamps for session 2005133...Done.\n",
      "Getting user turn timestamps for session 2008165...Done.\n",
      "Getting user turn timestamps for session 1003158...Done.\n",
      "Getting user turn timestamps for session 1002281...Done.\n",
      "Getting user turn timestamps for session 1002242...Done.\n",
      "Getting user turn timestamps for session 2001541...Done.\n",
      "Getting user turn timestamps for session 1003016...Done.\n",
      "Getting user turn timestamps for session 514928...Done.\n",
      "Getting user turn timestamps for session 513210...Done.\n",
      "Getting user turn timestamps for session 512508...Done.\n",
      "Getting user turn timestamps for session 514232...Done.\n",
      "Getting user turn timestamps for session 1002422...Done.\n",
      "Getting user turn timestamps for session 515289...Done.\n",
      "Getting user turn timestamps for session 2005036...Done.\n",
      "Getting user turn timestamps for session 1001918...Done.\n",
      "Getting user turn timestamps for session 2001521...Done.\n",
      "Getting user turn timestamps for session 515843...Done.\n"
     ]
    }
   ],
   "source": [
    "user_timestamp_suffix = '_user_timestamps.pkl'\n",
    "user_name_col = 0\n",
    "user_name = 'user'\n",
    "user_cs_dir = data_dir + 'user_cs/'\n",
    "user_pickle_dir = user_cs_dir + 'pickle_files/'\n",
    "\n",
    "for sid in session_ids:\n",
    "    print(\"Getting user turn timestamps for session %s...\" % (sid), end='')\n",
    "    with open(transcripts_dir + sid + txt_suffix, 'r') as fr:\n",
    "        rows = fr.readlines()\n",
    "        line = 0\n",
    "        tsec = {}\n",
    "        for i, r in enumerate(rows):\n",
    "            r = r.rstrip(\"\\n\")\n",
    "            r = r.split(\",\")\n",
    "            \n",
    "            if r[user_name_col].lower() == user_name:\n",
    "                # Picking end of turn (last column) as the time stamp for the turn\n",
    "                try:\n",
    "                    struct_t = time.strptime(r[-1].strip(), time_format)\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        struct_t = time.strptime(r[-1].strip(), time_alt_format)\n",
    "                    except ValueError:\n",
    "                        print(r)\n",
    "                    \n",
    "                tsec[i] = struct_t[min_id] * min_to_sec + struct_t[sec_id]\n",
    "\n",
    "    with open(user_pickle_dir + sid + user_timestamp_suffix, 'wb') as f:\n",
    "        pickle.dump(tsec, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(user_cs_dir)\n",
    "session_ids = []\n",
    "csv_suffix = '.csv'\n",
    "for file in all_files:\n",
    "    if file.endswith(csv_suffix):\n",
    "        session_ids.append(file.split(csv_suffix)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting user CSs for session 1001444...26\n",
      "Done.\n",
      "Getting user CSs for session 2016938...22\n",
      "Done.\n",
      "Getting user CSs for session 1001335...20\n",
      "Done.\n",
      "Getting user CSs for session 1002013...19\n",
      "Done.\n",
      "Getting user CSs for session 1001877...26\n",
      "Done.\n",
      "Getting user CSs for session 516417...33\n",
      "Done.\n",
      "Getting user CSs for session 2001299...34\n",
      "Done.\n",
      "Getting user CSs for session 1002648...29\n",
      "Done.\n",
      "Getting user CSs for session 515233...29\n",
      "Done.\n",
      "Getting user CSs for session 515151...30\n",
      "Done.\n",
      "Getting user CSs for session 516517...31\n",
      "Done.\n",
      "Getting user CSs for session 2004828...23\n",
      "Done.\n",
      "Getting user CSs for session 2002040...33\n",
      "Done.\n",
      "Getting user CSs for session 1001636...20\n",
      "Done.\n",
      "Getting user CSs for session 1002655...22\n",
      "Done.\n",
      "Getting user CSs for session 515762...29\n",
      "Done.\n",
      "Getting user CSs for session 516135...26\n",
      "Done.\n",
      "Getting user CSs for session 2005133...34\n",
      "Done.\n",
      "Getting user CSs for session 2008165...16\n",
      "Done.\n",
      "Getting user CSs for session 1003158...28\n",
      "Done.\n",
      "Getting user CSs for session 1002281...23\n",
      "Done.\n",
      "Getting user CSs for session 1002242...33\n",
      "Done.\n",
      "Getting user CSs for session 2001541...27\n",
      "Done.\n",
      "Getting user CSs for session 1003016...28\n",
      "Done.\n",
      "Getting user CSs for session 514928...27\n",
      "Done.\n",
      "Getting user CSs for session 513210...30\n",
      "Done.\n",
      "Getting user CSs for session 512508...36\n",
      "Done.\n",
      "Getting user CSs for session 514232...23\n",
      "Done.\n",
      "Getting user CSs for session 1002422...25\n",
      "Done.\n",
      "Getting user CSs for session 515289...11\n",
      "Done.\n",
      "Getting user CSs for session 2005036...33\n",
      "Done.\n",
      "Getting user CSs for session 1001918...31\n",
      "Done.\n",
      "Getting user CSs for session 2001521...25\n",
      "Done.\n",
      "Getting user CSs for session 515843...37\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "none_str = 'NONE'\n",
    "user_cs_suffix = '_user_cs.pkl'\n",
    "\n",
    "def remove_all(full_list, ele_rm):\n",
    "    trimmed_list = [ele for ele in full_list if ele != ele_rm]\n",
    "    if trimmed_list == []:\n",
    "        trimmed_list.append(none_str)\n",
    "    return trimmed_list\n",
    "\n",
    "for sid in session_ids:\n",
    "    print(\"Getting user CSs for session %s...\" % (sid), end='')\n",
    "    df = pd.read_csv(user_cs_dir + sid + csv_suffix, engine='python')\n",
    "    cols = df.columns.values.tolist()[1:]\n",
    "\n",
    "    for c in cols:\n",
    "        df[c].replace(1, c, inplace=True)\n",
    "    ind = df.index.values.tolist()\n",
    "    print(len(ind))\n",
    "    cs = {i: remove_all(df.loc[i].tolist()[1:], 0) for i in ind}\n",
    "    if sid == '510999':\n",
    "        print(cs)\n",
    "    \n",
    "    with open(user_pickle_dir + sid + user_cs_suffix, 'wb') as f:\n",
    "        pickle.dump(cs, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
